
Puzzles with UCT search
===========================
AHN, Mar 2020

Abstract
----------

The policy p is problem independent. v (the quality) is the interesting part.
We try reinforcement learning where v = exp(-lambda*remaining_effort). All else is more or less like Alpha Zero.

We start with simple puzzle states, which are close to the solution, and then scale up to harder initial configurations.

Policy and Value
------------------

The game result is just a way to get a quality measure for a two person game. In a puzzle, remaining effort is the natural choice. We want numbers between 0 and 1, where no remaining effort maps to 1.0, and large remaining effort maps to values close to 0. Intuitively, exp(-lambda * x) comes to mind. Say we want exp(-lambda * 20) = 0.5 as a first fit, so lambda ~ 0.035.

Policy p as usual, which means the normalized node counts beneath the children.

Parts
--------

Player
~~~~~~~~
p = Player( network, playouts)
move, newstate = p.move( state)

This class contains the UCT Search. Implement this first, see how it does if I feed it the networks trained in
07_valitershift.

Game
~~~~~
One puzzle solution

p = Player( network, playouts)
g = Game( p)
seq,movecount = g.play( random_shifts, movelimit) # Abort if player does not finish within limit.

For simplicity, just set the move limit to min( 100, 2 * random_shifts).

seq: List( state, p, v). The game class automatically populates v after the puzzle is solved and the number of moves is known.

The elements in seq are training examples.

Data Generator
~~~~~~~~~~~~~~~~
Continuously generates training inputs by playing games, only keeping the newest n training inputs. Does this by playing games and storing the resulting training data in a folder. After each game, check if there is a new network.

p = Player( network)
gen = DataGen( p, folder)
gen.run()

A training sample maps a state to a pair ([p_left,p_right,p_up,p_down], v)

Training Process
~~~~~~~~~~~~~~~~~~
Randomly sucks M training samples from folder, then trains on them in batches of size N. When done with all M samples, store the new network.
train.py --puzzlesize <int> --loadsize <int> --batchsize <int>
Ideas:
   - I could sort the input files and only train on a sample from the top n% to focus on harder inputs
   - Pick n randomly.

Match
~~~~~~~~
Matches two networks against each other in a tournament of N games. The result is appended to a json file. Fields are network hashes, number of games, number of wins for each player, number of aborted draws. No need to store the games themselves, I don't think I want to look at those.

Scripts
~~~~~~~~~
There will be three scripts:
- generate
- train
- match

Generate and train run forever until aborted.
Match completes one match and stores the result.


=== The End ===
