
Puzzles with UCT search
===========================
AHN, Mar 2020

Abstract
----------

Trying to adapt the AlphaZero approach to puzzles hits a snag because puzzles are not a fighting game between two opponents like chess or Go. How can you train v if there never is a win?

The idea is to turn any puzzle into a competition by letting two networks try to solve it, and the one with the better solution is called the winner.

We start with simple puzzle states, which are close to the solution, and then scale up to harder configurations.

Policy and Value
------------------
I thought policy was just a performance optimization because in Go, running the value net for each possible move is really expensive. Most puzzles do not have this issue. So you might try to just train the value, and use that to generate the policy.

However, this throws away most of the information gained during the UCT search. The policy is an attempt to train the results of the dynamic search effort into a static network. So next time around, you get the old answer straight from the net, and the same effort of search will explore further into the future. It seems stupid to throw that information away. Might be worth a try, though, just to make sure. It is also makes the code and the data structures a little simpler.

The game result is just a way to get a quality measure for a two person game. In a puzzle, remaining effort is the natural choice. We want numbers between 0 and 1, where remaining effort zero maps to 1, and large remaining effort maps to 0. exp(-lambda * x) comes to mind. Say exp(-lambda * 20) = 0.5 as a first fit, so lambda ~ 0.035.

So v = exp(-lambda * steps_to_solution) . p as usual, which means the normalized node counts beneath the children.

Parts
--------

Player
~~~~~~~~
p = Player( network, playouts)
move, newstate = p.move( state)

This class contains the UCTSearch. Implement this first, see how it does if I feed it the networks trained in
07_valitershift.

Game
~~~~~
One puzzle solution

p = Player( network, playouts)
g = Game( p)
seq,movecount = g.play( random_shifts, movelimit) # Abort if player does not finish within limit.

For simplicity, just set the move limit to min( 100, 2 * random_shifts).

seq: List( state, p, v). The game class automatically populates v after the puzzle is solved and the number of moves is known.

The elements in seq are training examples.

Data Generator
~~~~~~~~~~~~~~~~
Continuously generates training inputs by playing games, only keeping the newest n training inputs. Does this by playing games and storing the resulting training data in a folder. After each game, check if there is a new network.

p = Player( network)
gen = DataGen( p, folder)
gen.run()

A training sample maps a state to a pair ([p_left,p_right,p_up,p_down], v)

Training Process
~~~~~~~~~~~~~~~~~~
Randomly sucks M training samples from folder, then trains on them in batches of size N. When done with all M samples, store the new network.

Match
~~~~~~~~
Matches two networks against each other in a tournament of N games. The result is appended to a json file. Fields are network hashes, number of games, number of wins for each player, number of aborted draws. No need to store the games themselves, I don't think I want to look at those.

Scripts
~~~~~~~~~
There will be three scripts:
- generate
- train
- match

Generate and train run forever until aborted.
Match completes one match and stores the result.


=== The End ===
